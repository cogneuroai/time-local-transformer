{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Theoretical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 1\n",
    "vocab_size = 33278\n",
    "n_layer = 4\n",
    "n_head = 8\n",
    "n_embd = 368\n",
    "bias = False\n",
    "H = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we see: 18792288\n",
      "name                 params     ratio (%) \n",
      "embedding/token        12246304    65.1666\n",
      "embedding              12246304    65.1666\n",
      "recurrent/ih               1472     0.0078\n",
      "recurrent/hh               5888     0.0313\n",
      "recurrent/bias             2944     0.0157\n",
      "recurrent                 10304     0.0548\n",
      "attention/ln                368     0.0020\n",
      "attention/ln2               368     0.0020\n",
      "attention/kqv            406272     2.1619\n",
      "attention/proj           135424     0.7206\n",
      "attention                542432     2.8865\n",
      "mlp/ln                      368     0.0020\n",
      "mlp/ffw                  541696     2.8825\n",
      "mlp/proj                 541696     2.8825\n",
      "mlp                     1083760     5.7670\n",
      "block                   1636496     8.7083\n",
      "transformer             6545984    34.8334\n",
      "dense                         0     0.0000\n",
      "total                  18792288   100.0000\n"
     ]
    }
   ],
   "source": [
    "def params():\n",
    "    \"\"\" estimates the number of parameters in the model\"\"\"\n",
    "    out = OrderedDict()\n",
    "\n",
    "    # token and position embeddings\n",
    "    out['embedding/token'] = n_embd * vocab_size\n",
    "    out['embedding'] = out['embedding/token']\n",
    "\n",
    "    # recurrent units\n",
    "    out['recurrent/ih'] = n_embd * H  # input to hidden weights\n",
    "    out['recurrent/hh'] = n_embd * H * H  # hidden to hidden weights\n",
    "    out['recurrent/bias'] = n_embd * 2 * H  # 2 bias terms per RNN unit, each of size H, and n_embd such units\n",
    "    out['recurrent'] = (out['recurrent/ih'] + out['recurrent/hh'] + out['recurrent/bias'])\n",
    "\n",
    "    # attention blocks\n",
    "    out['attention/ln'] = n_embd # note, bias=False in our LN\n",
    "    out['attention/ln2'] = n_embd # note, bias=False in our LN\n",
    "    out['attention/kqv'] = n_embd * 3*n_embd\n",
    "    out['attention/proj'] = n_embd**2\n",
    "    out['attention'] = out['attention/ln'] + out['attention/ln2'] + out['attention/kqv'] + out['attention/proj']\n",
    "\n",
    "    # MLP blocks\n",
    "    ffw_size = 4*n_embd # feed forward size\n",
    "    out['mlp/ln'] = n_embd\n",
    "    out['mlp/ffw'] = n_embd * ffw_size\n",
    "    out['mlp/proj'] = ffw_size * n_embd\n",
    "    out['mlp'] = out['mlp/ln'] + out['mlp/ffw'] + out['mlp/proj']\n",
    "    \n",
    "    # the transformer and the rest of it\n",
    "    out['block'] = out['attention'] + out['mlp'] + out['recurrent']\n",
    "    out['transformer'] = n_layer * out['block']\n",
    "    #out['ln_f'] = n_embd # final layernorm\n",
    "    out['dense'] = 0 # 0 because of parameter sharing. This layer uses the weights from the embedding layer\n",
    "\n",
    "    # total\n",
    "    out['total'] = out['embedding'] + out['transformer'] # + out['ln_f'] + out['dense']\n",
    "\n",
    "    return out\n",
    "\n",
    "# compare our param count to that reported by PyTorch\n",
    "p = params()\n",
    "params_total = p['total']\n",
    "print(f\"we see: {params_total}\")\n",
    "# create a header\n",
    "print(f\"{'name':20s} {'params':10s} {'ratio (%)':10s}\")\n",
    "for k,v in p.items():\n",
    "    print(f\"{k:20s} {v:10d} {v/params_total*100:10.4f}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f5833218766b48e6e35e4452ee875aac0e2188d05bbe5298f2c62b79f08b222"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
